{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python数据模型\n",
    "\n",
    "## 数据模型\n",
    "Python数据模型对Python的整体框架进行了描述，其规范了Python构建模块的接口，包括且不限于序列、迭代器、函数、类以及上下文管理器\n",
    "\n",
    "在Python中一些特殊方法会以两个下划线开始并以两个下划线结束，因此这些方法也被称为双下方法(dunder method)，这些特殊方法提供了自定义类与下述的功能交互的“接口”：\n",
    "* 迭代\n",
    "* 集合类\n",
    "* 属性访问\n",
    "* **运算符重载**\n",
    "* 函数和方法的调用\n",
    "* 对象的创建和销毁\n",
    "* 字符串表示形式和格式化\n",
    "* 管理上下文(with)\n",
    "\n",
    "最为简单而常用的用法就是len()函数，对于len()函数，其默认会调用对应的\\_\\_len\\_\\_()方法。在自定义的类中则可以重载这一方法。这使得Python自带的数据结构以及自定义的类能够使用相同的函数执行类似的功能(一致性)。即这些特殊方法在保持内置类型的效率以及语言的一致性之间找到了一个平衡点。\n",
    "\n",
    "显然，这样的处理是有相当意义的。例如对于一些自定义的运算，若想定义自定义类的运算，最为直接的方法就是定义对应的函数，但是Python允许通过对对应的特殊方法进行重载实现运算符的重载。例如numpy中既可以使用dot()进行点积运算(自定义函数)，也可以直接使用*符号(运算符重载，具体实现为重载\\_\\_mul\\_\\_方法)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简单示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "对len()函数的重载:\n",
      "52\n",
      "\n",
      "根据索引0取值:\n",
      "Card(rank='2', suit='spades')\n",
      "\n",
      "切片:\n",
      "[Card(rank='2', suit='spades'), Card(rank='3', suit='spades'), Card(rank='4', suit='spades')]\n",
      "\n",
      "迭代:\n",
      "Card(rank='2', suit='spades')\n",
      "Card(rank='3', suit='spades')\n",
      "Card(rank='4', suit='spades')\n",
      "Card(rank='5', suit='spades')\n",
      "Card(rank='6', suit='spades')\n",
      "Card(rank='7', suit='spades')\n",
      "Card(rank='8', suit='spades')\n",
      "Card(rank='9', suit='spades')\n",
      "Card(rank='10', suit='spades')\n",
      "Card(rank='J', suit='spades')\n",
      "Card(rank='Q', suit='spades')\n",
      "Card(rank='K', suit='spades')\n",
      "Card(rank='A', suit='spades')\n",
      "Card(rank='2', suit='diamonds')\n",
      "Card(rank='3', suit='diamonds')\n",
      "Card(rank='4', suit='diamonds')\n",
      "Card(rank='5', suit='diamonds')\n",
      "Card(rank='6', suit='diamonds')\n",
      "Card(rank='7', suit='diamonds')\n",
      "Card(rank='8', suit='diamonds')\n",
      "Card(rank='9', suit='diamonds')\n",
      "Card(rank='10', suit='diamonds')\n",
      "Card(rank='J', suit='diamonds')\n",
      "Card(rank='Q', suit='diamonds')\n",
      "Card(rank='K', suit='diamonds')\n",
      "Card(rank='A', suit='diamonds')\n",
      "Card(rank='2', suit='clubs')\n",
      "Card(rank='3', suit='clubs')\n",
      "Card(rank='4', suit='clubs')\n",
      "Card(rank='5', suit='clubs')\n",
      "Card(rank='6', suit='clubs')\n",
      "Card(rank='7', suit='clubs')\n",
      "Card(rank='8', suit='clubs')\n",
      "Card(rank='9', suit='clubs')\n",
      "Card(rank='10', suit='clubs')\n",
      "Card(rank='J', suit='clubs')\n",
      "Card(rank='Q', suit='clubs')\n",
      "Card(rank='K', suit='clubs')\n",
      "Card(rank='A', suit='clubs')\n",
      "Card(rank='2', suit='hearts')\n",
      "Card(rank='3', suit='hearts')\n",
      "Card(rank='4', suit='hearts')\n",
      "Card(rank='5', suit='hearts')\n",
      "Card(rank='6', suit='hearts')\n",
      "Card(rank='7', suit='hearts')\n",
      "Card(rank='8', suit='hearts')\n",
      "Card(rank='9', suit='hearts')\n",
      "Card(rank='10', suit='hearts')\n",
      "Card(rank='J', suit='hearts')\n",
      "Card(rank='Q', suit='hearts')\n",
      "Card(rank='K', suit='hearts')\n",
      "Card(rank='A', suit='hearts')\n",
      "\n",
      "随机选择:\n",
      "Card(rank='A', suit='hearts')\n",
      "\n",
      "对sorted函数的支持:\n",
      "Card(rank='2', suit='clubs')\n",
      "Card(rank='2', suit='diamonds')\n",
      "Card(rank='2', suit='hearts')\n",
      "Card(rank='2', suit='spades')\n",
      "Card(rank='3', suit='clubs')\n",
      "Card(rank='3', suit='diamonds')\n",
      "Card(rank='3', suit='hearts')\n",
      "Card(rank='3', suit='spades')\n",
      "Card(rank='4', suit='clubs')\n",
      "Card(rank='4', suit='diamonds')\n",
      "Card(rank='4', suit='hearts')\n",
      "Card(rank='4', suit='spades')\n",
      "Card(rank='5', suit='clubs')\n",
      "Card(rank='5', suit='diamonds')\n",
      "Card(rank='5', suit='hearts')\n",
      "Card(rank='5', suit='spades')\n",
      "Card(rank='6', suit='clubs')\n",
      "Card(rank='6', suit='diamonds')\n",
      "Card(rank='6', suit='hearts')\n",
      "Card(rank='6', suit='spades')\n",
      "Card(rank='7', suit='clubs')\n",
      "Card(rank='7', suit='diamonds')\n",
      "Card(rank='7', suit='hearts')\n",
      "Card(rank='7', suit='spades')\n",
      "Card(rank='8', suit='clubs')\n",
      "Card(rank='8', suit='diamonds')\n",
      "Card(rank='8', suit='hearts')\n",
      "Card(rank='8', suit='spades')\n",
      "Card(rank='9', suit='clubs')\n",
      "Card(rank='9', suit='diamonds')\n",
      "Card(rank='9', suit='hearts')\n",
      "Card(rank='9', suit='spades')\n",
      "Card(rank='10', suit='clubs')\n",
      "Card(rank='10', suit='diamonds')\n",
      "Card(rank='10', suit='hearts')\n",
      "Card(rank='10', suit='spades')\n",
      "Card(rank='J', suit='clubs')\n",
      "Card(rank='J', suit='diamonds')\n",
      "Card(rank='J', suit='hearts')\n",
      "Card(rank='J', suit='spades')\n",
      "Card(rank='Q', suit='clubs')\n",
      "Card(rank='Q', suit='diamonds')\n",
      "Card(rank='Q', suit='hearts')\n",
      "Card(rank='Q', suit='spades')\n",
      "Card(rank='K', suit='clubs')\n",
      "Card(rank='K', suit='diamonds')\n",
      "Card(rank='K', suit='hearts')\n",
      "Card(rank='K', suit='spades')\n",
      "Card(rank='A', suit='clubs')\n",
      "Card(rank='A', suit='diamonds')\n",
      "Card(rank='A', suit='hearts')\n",
      "Card(rank='A', suit='spades')\n"
     ]
    }
   ],
   "source": [
    "# 简单示例\n",
    "# 自定义类型实现了__len__以及__gititem__的重载\n",
    "# 这使得类FrenchDeck能够直接使用Python内置len()函数以及[]操作\n",
    "# 执行类似于Python内置类型\"list\"能够执行的功能\n",
    "\n",
    "import collections\n",
    "\n",
    "Card = collections.namedtuple(\"Card\", [\"rank\", \"suit\"])\n",
    "\n",
    "class FrenchDeck:\n",
    "    ranks = [str(n) for n in range(2, 11)] + list(\"JQKA\")\n",
    "    suits = \"spades diamonds clubs hearts\".split()\n",
    "\n",
    "    def __init__(self):\n",
    "        self._cards = [Card(rank, suit) for suit in self.suits \n",
    "                                        for rank in self.ranks]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._cards)\n",
    "    \n",
    "    def __getitem__(self, position):\n",
    "        return self._cards[position]\n",
    "\n",
    "\n",
    "# -------  实例  -------\n",
    "deck = FrenchDeck()\n",
    "\n",
    "# -------  对len()的重载  -------\n",
    "print(\"\\n对len()函数的重载:\")\n",
    "print(len(deck))\n",
    "\n",
    "# -------  对[]操作的重载 -------\n",
    "# 根据索引取值\n",
    "print(\"\\n根据索引0取值:\")\n",
    "print(deck[0])\n",
    "# 切片\n",
    "print(\"\\n切片:\")\n",
    "print(deck[:3])\n",
    "# 迭代\n",
    "print(\"\\n迭代:\")\n",
    "for card in deck:\n",
    "    print(card)\n",
    "# 对其他标准库的支持\n",
    "print(\"\\n随机选择:\")\n",
    "import random\n",
    "print(random.choice(deck))\n",
    "\n",
    "# -------  重排序  -------\n",
    "suit_values = dict(spades=3, hearts=2, diamonds=1, clubs=0)\n",
    "def spades_high(card):\n",
    "    rank_value = FrenchDeck.ranks.index(card.rank)\n",
    "    return rank_value * len(suit_values) + suit_values[card.suit]\n",
    "# 对sorted函数的支持\n",
    "print(\"\\n对sorted函数的支持:\")\n",
    "for card in sorted(deck, key=spades_high):\n",
    "    print(card)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述例子显示，自定义类完全可以有与内置类相似的功能。\n",
    "值得注意的是双下方法通常不应该被直接调用，例如对于上述例子，通常不应该尝试使用deck.\\_\\_len\\_\\_()。通过内置函数使用特殊方法往往是最好的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 运算符重载\n",
    "\n",
    "通过重载特殊方法对运算符进行重载是有好处的。其最明显的好处就是能够保证一致性。例如对于实数的二元加法可以使用运算符+。现在想实现向量的加法，一个选择是创建自定义函数，例如定义函数vector_add(x_1, x_2)；另一个选择就是进行运算符重载，通过运算符重载实现使用运算符+对向量进行加法运算的目的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "加法:\n",
      "Vector(4, 5)\n",
      "\n",
      "绝对值:\n",
      "4.47213595499958\n",
      "\n",
      "数乘:\n",
      "Vector(6, 12)\n"
     ]
    }
   ],
   "source": [
    "from math import hypot\n",
    "\n",
    "class Vector:\n",
    "\n",
    "    def __init__(self, x_1=0, x_2=0):\n",
    "        self.x_1 = x_1\n",
    "        self.x_2 = x_2\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Vector({}, {})\".format(self.x_1, self.x_2)\n",
    "    \n",
    "    def __abs__(self):\n",
    "        return hypot(self.x_1, self.x_2)\n",
    "    \n",
    "    def __bool__(self):\n",
    "        return bool(abs(self))\n",
    "\n",
    "    def __add__(self, other):\n",
    "        x_1 = self.x_1 + other.x_1\n",
    "        x_2 = self.x_2 + other.x_2\n",
    "        return Vector(x_1, x_2)\n",
    "    \n",
    "    def __mul__(self, scalar):\n",
    "        return Vector(self.x_1 * scalar, self.x_2 * scalar)\n",
    "\n",
    "\n",
    "# -------  实例  -------\n",
    "v1 = Vector(2, 4)\n",
    "v2 = Vector(2, 1)\n",
    "\n",
    "# -------  加法重载 -------  \n",
    "print(\"\\n加法:\")\n",
    "print(v1 + v2)\n",
    "\n",
    "# -------  绝对值重载  -------  \n",
    "print(\"\\n绝对值:\")\n",
    "print(abs(v1))\n",
    "\n",
    "# -------  数乘重载 -------\n",
    "print(\"\\n数乘:\")\n",
    "print(v1 * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\_\\_repr\\_\\_ & \\_\\_str\\_\\_\n",
    "上述定义的Vector除了实现一些运算符的重载外还实现了方法\\_\\_repr\\_\\_，这个方法定义了一个对象的字符串形式。当需要打印该对象时，\\_\\_repr\\_\\_会被调用。而若没有定义\\_\\_repr\\_\\_则会打印该对象所在的内存地址。与\\_\\_repr\\_\\_类似的还有方法\\_\\_str\\_\\_，不同在于后者在被print输出或者str()函数转换为字符串时调用。当没有实现\\_\\_str\\_\\_但使用print输出该实例时，会自动调用\\_\\_repr\\_\\_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "print函数输出\n",
      "向量v1: <__main__.ReprVector1 object at 0x00000211E8736C08>\n",
      "向量v2: Vector(1, 1)\n",
      "向量v3: Vector(1, 1)\n",
      "\n",
      "直接打印实例:\n",
      "向量v3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ReprVector3 at 0x211e8cf07c8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector 1\n",
    "# 不实现__repr__以及__str__\n",
    "class ReprVector1:\n",
    "\n",
    "    def __init__(self, x_1=0, x_2=0):\n",
    "        self.x_1 = x_1\n",
    "        self.x_2 = x_2\n",
    "\n",
    "# vector2\n",
    "# 实现__repr__，但不实现__str__\n",
    "class ReprVector2:\n",
    "\n",
    "    def __init__(self, x_1=0, x_2=0):\n",
    "        self.x_1 = x_1\n",
    "        self.x_2 = x_2\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Vector({}, {})\".format(self.x_1, self.x_2)\n",
    "\n",
    "# vector3\n",
    "# 实现__str__，但不实现__repr__\n",
    "class ReprVector3:\n",
    "\n",
    "    def __init__(self, x_1=0, x_2=0):\n",
    "        self.x_1 = x_1\n",
    "        self.x_2 = x_2\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Vector({}, {})\".format(self.x_1, self.x_2)\n",
    "\n",
    "\n",
    "v1 = ReprVector1(1, 1)\n",
    "v2 = ReprVector2(1, 1)\n",
    "v3 = ReprVector3(1, 1)\n",
    "\n",
    "# print 输出\n",
    "print(\"\\nprint函数输出\")\n",
    "print(\"向量v1:\", v1)\n",
    "print(\"向量v2:\", v2)\n",
    "print(\"向量v3:\", v3)\n",
    "\n",
    "# 直接打印实例\n",
    "print(\"\\n直接打印实例:\")\n",
    "print(\"向量v3\")\n",
    "v3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，上述的Vector3在控制台直接打印时没有正确输出其对应的\"字符串表示形式\"，但是在print函数中，Vector3正确输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\_\\_bool\\_\\_\n",
    "\n",
    "bool()函数的返回结果同样可以自定义。当使用bool()函数时，会自动调用\\_\\_bool\\_\\_()方法，若不存在\\_\\_bool\\_\\_()方法则会尝试调用\\_\\_len\\_\\_()，若\\_\\_len\\_\\_()也没有实现则会默认为真。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "class BoolVector1:\n",
    "\n",
    "    def __init__(self, x_1=0, x_2=0):\n",
    "        self.x_1 = x_1\n",
    "        self.x_2 = x_2\n",
    "\n",
    "class BoolVector2:\n",
    "\n",
    "    def __init__(self, x_1=0, x_2=0):\n",
    "        self.x_1 = x_1\n",
    "        self.x_2 = x_2\n",
    "\n",
    "    def __bool__(self):\n",
    "        return bool(self.x_1 or self.x_2)\n",
    "\n",
    "class BoolVector3:\n",
    "\n",
    "    def __init__(self, x_1=0, x_2=0):\n",
    "        self.x_1 = x_1\n",
    "        self.x_2 = x_2\n",
    "    \n",
    "    def __len__(self):\n",
    "        return bool(self.x_1 or self.x_2)\n",
    "\n",
    "class BoolVector4:\n",
    "\n",
    "    def __init__(self, x_1=0, x_2=0):\n",
    "        self.x_1 = x_1\n",
    "        self.x_2 = x_2\n",
    "    \n",
    "    def __len__(self):\n",
    "        return bool(2)\n",
    "\n",
    "\n",
    "v1 = BoolVector1(0, 0)\n",
    "v2 = BoolVector2(0, 0)\n",
    "v3 = BoolVector3(0, 0)\n",
    "v4 = BoolVector4(0, 0)\n",
    "\n",
    "print(bool(v1))\n",
    "print(bool(v2))\n",
    "print(bool(v3))\n",
    "print(bool(v4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，\n",
    "\n",
    "BoolVector1既没有实现\\_\\_bool\\_\\_也没有实现\\_\\_len\\_\\_，因此输出为True\n",
    "\n",
    "BoolVector2实现了\\_\\_bool\\_\\_，没有实现\\_\\_len\\_\\_，因此输出为False\n",
    "\n",
    "BoolVector3没有实现\\_\\_bool\\_\\_，但是实现了\\_\\_len\\_\\_，并判断向量是否为0，因此输出为False\n",
    "\n",
    "BoolVector4没有实现\\_\\_bool\\_\\_，但是实现了\\_\\_len\\_\\_，并始终返回bool(2)，因此输出为True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一个例子：自动微分\n",
    "\n",
    "自动微分的实现可以被视为上述运算符重载的一个特殊应用。对于自动微分来说，每一次运算不仅要计算得到运算结果，还需要记录梯度，因此需要对运算符进行重载以实现这一功能需求。\n",
    "\n",
    "总的来说，对于自动微分需要重载大量运算符，下面就是带有自动微分功能的一个基本实现。这里实现的是自动微分的反向模式(Reverse mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "输出tensor对tensor1的偏导:\n",
      "[[3. 3.]\n",
      " [3. 3.]]\n",
      "\n",
      "输出tensor对tensor2的偏导:\n",
      "[[12.]\n",
      " [12.]]\n",
      "\n",
      "输出tensor对tensor3的偏导:\n",
      "[[4.]\n",
      " [4.]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "random.seed(1024)\n",
    "np.random.seed(1024)\n",
    "\n",
    "def as_tensor(obj):\n",
    "    if not isinstance(obj, MyTensor):\n",
    "        obj = MyTensor(obj)\n",
    "    return obj\n",
    "\n",
    "def build_binary_ops_result_tensor(tensor1, tensor2, grad_func_tensor1, grad_func_tensor2, values):\n",
    "    \"\"\"\n",
    "    建立二元运算结果的tensor\n",
    "    \"\"\"\n",
    "    requires_grad = tensor1.requires_grad or tensor2.requires_grad\n",
    "    \n",
    "    dependency = list()\n",
    "    if tensor1.requires_grad:\n",
    "        dependency.append({\"tensor\": tensor1, \"grad_func\": grad_func_tensor1})\n",
    "    if tensor2.requires_grad:\n",
    "        dependency.append({\"tensor\": tensor2, \"grad_func\": grad_func_tensor2})\n",
    "\n",
    "    return MyTensor(values, requires_grad, dependency)\n",
    "\n",
    "def build_unary_ops_result_tensor(tensor, grad_func, values):\n",
    "    \"\"\"\n",
    "    建立一元运算结果的tensor\n",
    "    \"\"\"\n",
    "\n",
    "    requires_grad = tensor.requires_grad\n",
    "    dependency = list()\n",
    "\n",
    "    if tensor.requires_grad:\n",
    "        dependency.append({\"tensor\": tensor, \"grad_func\": grad_func})\n",
    "    \n",
    "    return MyTensor(values, requires_grad, dependency) \n",
    "\n",
    "\n",
    "class MyTensor(object):\n",
    "\n",
    "    def __init__(self, values, requires_grad=False, dependency=None):\n",
    "        self._values = np.array(values)\n",
    "        self._shape = self._values.shape\n",
    "\n",
    "        self.grad = None\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "        if self.requires_grad:\n",
    "            self.zero_grad()\n",
    "\n",
    "        if dependency is None:\n",
    "            self.dependency = list()\n",
    "        else:\n",
    "            self.dependency = dependency\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.grad = np.zeros(self._shape)\n",
    "    \n",
    "    @property\n",
    "    def values(self):\n",
    "        return self._values\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._shape\n",
    "    \n",
    "    @values.setter\n",
    "    def values(self, values):\n",
    "        self._values = np.array(values)\n",
    "        self.grad = None\n",
    "\n",
    "    def backward(self, grad=None):\n",
    "        \"\"\"\n",
    "        反向传播核心代码，累计当前tensor的梯度，同时反向传播梯度\n",
    "        \"\"\"\n",
    "        assert self.requires_grad, \"Call backward() on a non-requires-grad tensor\"\n",
    "\n",
    "        if grad is None:\n",
    "            grad = 1.0\n",
    "        grad = np.array(grad)\n",
    "\n",
    "        # 梯度叠加\n",
    "        self.grad += grad\n",
    "\n",
    "        # 反向传播梯度到直接依赖的运算\n",
    "        for dep in self.dependency:\n",
    "            # 注意这里是直接将梯度传播到依赖的运算，而不是传播梯度叠加的结果\n",
    "            # 其原因在于这里使用的是深度优先遍历\n",
    "            # 由于在反向传播梯度后，会进行直接依赖的运算反向传播，因此是深度优先\n",
    "            # 这么一来在遍历完成后，所有经过当前运算传递到依赖运算的路径均会被搜索到\n",
    "            \n",
    "            # 比如\n",
    "            # 假设当前节点前有两个后继节点\n",
    "            # 那么反向传播流程为\n",
    "\n",
    "            # 梯度清零\n",
    "            # 后继节点一反向传播，当前节点梯度+grad_1\n",
    "            # 当前节点继续反向传播，前置节点梯度+grad_1\n",
    "            # 另一条路径：后继节点二反向传播，当前节点梯度+grad_2\n",
    "            # 当前节点继续反向传播，前置节点梯度+grad_2\n",
    "\n",
    "            # 若在上述过程中每次传递的是累计梯度，则会导致前置节点梯度+grad_1+(grad_1+grad_2)\n",
    "            # 即在第二条路径中对第一条路径的梯度又叠加了一次\n",
    "            grad_cal_method_for_dep = dep[\"grad_func\"](grad)\n",
    "            # 迭代调用直接依赖的运算的反向传播\n",
    "            dep[\"tensor\"].backward(grad_cal_method_for_dep)\n",
    "    \n",
    "    # 运算重载，对于反向传播，其不仅要计算值，还需要计算梯度\n",
    "    # 重载运算\n",
    "\n",
    "    # 二元运算：\n",
    "    # 矩阵乘法：左乘、右乘、原位乘\n",
    "    # 加法：左加、右加、原位加\n",
    "    # 减法：左减、右减、原位减\n",
    "    # 按位乘法：左乘、右乘、原位乘\n",
    "\n",
    "    # 一元运算：\n",
    "    # 取反\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        右加法重载\n",
    "        \n",
    "        return: self + other\n",
    "        \"\"\"\n",
    "        return self._add(self, as_tensor(other))\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        \"\"\"\n",
    "        左加法重载\n",
    "\n",
    "        return: other + self\n",
    "        \"\"\"\n",
    "        return self._add(as_tensor(other), self)\n",
    "    \n",
    "    def __iadd__(self, other):\n",
    "        \"\"\"\n",
    "        原位加法重载\n",
    "\n",
    "        self += other\n",
    "\n",
    "        原位加法不会生成新的节点，因此不会记录梯度\n",
    "        \"\"\"\n",
    "        self.values = self.values + as_tensor(other).values\n",
    "        return self\n",
    "\n",
    "    def _add(self, tensor1, tensor2):\n",
    "        \"\"\"\n",
    "        运算为：c = a + b\n",
    "\n",
    "        Dc/Da = 1\n",
    "        Dc/Db = 1\n",
    "\n",
    "        在本运算中，要求 a.shape == b.shape == c.shape\n",
    "        或者使用numpy默认的broadcast机制\n",
    "\n",
    "        对于维度匹配的加法，梯度直接传递即可\n",
    "        对于维度无法匹配需要采用broadcast机制进行计算的加法，需要对维度进行处理\n",
    "\n",
    "        broadcast机制：\n",
    "        这里仅考虑二维的情况\n",
    "\n",
    "        1. 对于形如 (m, n) + (n, ) -> (m, n)形式的broadcast:\n",
    "        对于第一个加数，将计算结果的梯度直接赋予当前加数即可\n",
    "        对于第二个加数，其需要将计算结果的梯度沿第一维度叠加，然后赋予当前加数\n",
    "\n",
    "        2. 对于形如(m, n) + (1, n) -> (m, n)形式的broadcast：\n",
    "        对于第一个加数，将计算结果的梯度直接赋予当前加数即可\n",
    "        对于第二个加数，其需要将计算结果的梯度沿第一维度叠加，并保留维度，然后赋予当前加数\n",
    "        \"\"\"\n",
    "        _result = tensor1.values + tensor2.values\n",
    "\n",
    "        def grad_func_tensor1(grad):\n",
    "\n",
    "            # 处理第一种情况\n",
    "            # 此时，若当前tensor的维度要小于grad的维度，则说明存在第一种broadcast\n",
    "            for _ in range(grad.ndim - tensor1.values.ndim):\n",
    "                grad = grad.sum(axis=0)\n",
    "            # 处理第二种情况\n",
    "            # 此时，若当前tensor的维度等于grad的维度，但是存在为1的维度，则说明可能是第二种broadcast\n",
    "            for i, dim in enumerate(tensor1.values.shape):\n",
    "                if dim == 1:\n",
    "                    grad = grad.sum(axis=i, keepdims=True)\n",
    "            return grad\n",
    "\n",
    "        def grad_func_tensor2(grad):\n",
    "            # 同上\n",
    "            for _ in range(grad.ndim - tensor2.values.ndim):\n",
    "                grad = grad.sum(axis=0)\n",
    "            for i, dim in enumerate(tensor2.values.shape):\n",
    "                if dim == 1:\n",
    "                    grad = grad.sum(axis=i, keepdims=True)\n",
    "            return grad\n",
    "        \n",
    "        return build_binary_ops_result_tensor(tensor1, tensor2, grad_func_tensor1, grad_func_tensor2, _result)\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        \"\"\"\n",
    "        矩阵右乘\n",
    "\n",
    "        return self @ other\n",
    "        \"\"\"\n",
    "        return self._matmul(self, as_tensor(other))\n",
    "    \n",
    "    def __rmatmul__(self, other):\n",
    "        \"\"\"\n",
    "        矩阵左乘\n",
    "        \n",
    "        return other @ self\n",
    "        \"\"\"\n",
    "        return self._matmul(as_tensor(other), self)\n",
    "    \n",
    "    def __imatmul__(self, other):\n",
    "        \"\"\"\n",
    "        原位乘法\n",
    "        self @= other\n",
    "        \"\"\"\n",
    "        self.values = self.values @ as_tensor(other).values\n",
    "        return self\n",
    "\n",
    "    def _matmul(self, tensor1, tensor2):\n",
    "        \"\"\"\n",
    "        运算为: c = a @ b\n",
    "\n",
    "        Dc/Da = grad @ b.T\n",
    "        Dc/Db = a.T @ grad\n",
    "        \"\"\"\n",
    "        if tensor1.shape[1] != tensor2.shape[0]:\n",
    "            raise RuntimeError(\"RuntimeError: size mismatch, m1: {}, m2: {}\".format(tensor1.shape, tensor2.shape))\n",
    "\n",
    "        _result = tensor1.values @ tensor2.values\n",
    "        def grad_func_tensor1(grad):\n",
    "            return grad @ tensor2.values.T\n",
    "        \n",
    "        def grad_func_tensor2(grad):\n",
    "            return tensor1.values.T @ grad\n",
    "\n",
    "        return build_binary_ops_result_tensor(tensor1, tensor2, grad_func_tensor1, grad_func_tensor2, _result)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        \"\"\"\n",
    "        按位右乘\n",
    "\n",
    "        reture self * other\n",
    "        \"\"\"\n",
    "        return self._mul(self, as_tensor(other))\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        \"\"\"\n",
    "        按位左乘\n",
    "\n",
    "        return other * self\n",
    "        \"\"\"\n",
    "        return self._mul(as_tensor(other), self)\n",
    "    \n",
    "    def __imul__(self, other):\n",
    "        \"\"\"\n",
    "        原位按位乘\n",
    "\n",
    "        self *= other\n",
    "        \"\"\"\n",
    "        self.values = self.values * as_tensor(other).values\n",
    "        return self\n",
    "\n",
    "    def _mul(self, tensor1, tensor2):\n",
    "        \"\"\"\n",
    "        运算为: c = a * b\n",
    "\n",
    "        Dc/Da = b\n",
    "        Dc/Db = a\n",
    "\n",
    "        在本运算中，要求 a.shape == b.shape == c.shape\n",
    "        或者使用numpy默认的broadcast机制\n",
    "\n",
    "        对于维度匹配的加法，梯度直接传递即可\n",
    "        对于维度无法匹配需要采用broadcast机制进行计算的加法，需要对维度进行处理\n",
    "\n",
    "        由于\n",
    "        Dc/Da = b\n",
    "        Dc/Db = a\n",
    "        因此在计算梯度时，不同于add，按位乘法需要依赖numpy的broadcast首先计算梯度值，然后处理维度问题\n",
    "        即：\n",
    "        tensor1.grad = grad * tensor2.grad\n",
    "        tensor2.grad = grad * tensor1.grad\n",
    "\n",
    "        维度处理同add\n",
    "        \"\"\"\n",
    "        _result = tensor1.values * tensor2.values\n",
    "\n",
    "        def grad_func_tensor1(grad):\n",
    "            grad = grad * tensor2.values\n",
    "\n",
    "            # 处理第一种情况\n",
    "            # 此时，若当前tensor的维度要小于grad的维度，则说明存在第一种broadcast\n",
    "            for _ in range(grad.ndim - tensor1.values.ndim):\n",
    "                grad = grad.sum(axis=0)\n",
    "            # 处理第二种情况\n",
    "            # 此时，若当前tensor的维度等于grad的维度，但是存在为1的维度，则说明可能是第二种broadcast\n",
    "            for i, dim in enumerate(tensor1.values.shape):\n",
    "                if dim == 1:\n",
    "                    grad = grad.sum(axis=i, keepdims=True)\n",
    "            return grad\n",
    "\n",
    "        def grad_func_tensor2(grad):\n",
    "            grad = grad * tensor1.values\n",
    "            # 同上\n",
    "            for _ in range(grad.ndim - tensor2.values.ndim):\n",
    "                grad = grad.sum(axis=0)\n",
    "            for i, dim in enumerate(tensor2.values.shape):\n",
    "                if dim == 1:\n",
    "                    grad = grad.sum(axis=i, keepdims=True)\n",
    "            return grad\n",
    "        \n",
    "        return build_binary_ops_result_tensor(tensor1, tensor2, grad_func_tensor1, grad_func_tensor2, _result)\n",
    "\n",
    "\n",
    "# 假设进行如下的运算\n",
    "# x1 : [2, 2]\n",
    "# x2 : [2, 1]\n",
    "# x3 : [2, 1]\n",
    "# y = x3 * (x1 @ (x2 + 1))\n",
    "\n",
    "# y 对x1的偏导为：x3 @ (x2 + 1)^T\n",
    "# y 对x2的偏导为：x1^T @ x3\n",
    "# y 对x3的偏导为：x1 @ (x2 + 1)\n",
    "\n",
    "tensor1 = MyTensor(np.ones((2, 2)) * 2, requires_grad=True)\n",
    "tensor2 = MyTensor(np.zeros((2, 1)), requires_grad=True)\n",
    "tensor3 = MyTensor(np.ones((2, 1)) * 3, requires_grad=True)\n",
    "\n",
    "output_tensor = tensor3 * (tensor1 @ (tensor2 + 1))\n",
    "\n",
    "# 梯度清零\n",
    "output_tensor.zero_grad()\n",
    "# 梯度反向传播\n",
    "output_tensor.backward()\n",
    "\n",
    "print(\"\\n输出tensor对tensor1的偏导:\")\n",
    "print(tensor1.grad)\n",
    "print(\"\\n输出tensor对tensor2的偏导:\")\n",
    "print(tensor2.grad)\n",
    "print(\"\\n输出tensor对tensor3的偏导:\")\n",
    "print(tensor3.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述定义的tensor类，重载了各类运算符，这使得在计算运算结果的同时也会记录相应的梯度，实现自动微分功能。相对更完整的自动微分实现（基本的激活函数和求和等操作）以及应用(全连接网络)可以参考[基本自动微分](https://github.com/koolo233/NeuralNetworks/blob/main/BasicAutomaticDifferentiation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "“通过实现特殊方法，自定义数据类型可以表现得跟内置类型一样，从而让我们写出更具表达力的代码——或者说，更具Python风格的代码”\n",
    "\n",
    "确实相当方便，并且能够有效降低记忆成本"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c138c3a39b15f82bd4a9598693589d2dc2de979d592ed7357973539f0f36bd2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('flyai_pytorch1_5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
